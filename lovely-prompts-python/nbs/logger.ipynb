{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import List, Dict, Any, Optional, Union, Generator\n",
    "\n",
    "from websockets.sync.client import connect as ws_connect\n",
    "\n",
    "from lovely_prompts.utils import max_tokens_for_model\n",
    "\n",
    "from lovely_prompts_server.models import (\n",
    "    WSMessage,\n",
    "    ChatMessage,\n",
    "    #\n",
    "    ChatPrompt,\n",
    "    ChatResponse,\n",
    "    CompletionPrompt,\n",
    "    CompletionResponse,\n",
    "    #\n",
    "    ChatPromptModel,\n",
    "    CompletionPromptModel,\n",
    "    ChatResponseModel,\n",
    "    CompletionResponseModel,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialSession(requests.Session):\n",
    "    def __init__(self, base_url: Union[str, None], project: Union[str, None]):\n",
    "        super().__init__()\n",
    "        self.base_url = base_url\n",
    "        self.project = project\n",
    "\n",
    "    def request(self, method, url, **kwargs):\n",
    "        if self.project is not None:\n",
    "            params = {\"project\": self.project} | kwargs.get(\"params\", {})\n",
    "            kwargs[\"params\"] = params\n",
    "        return super().request(method, self.base_url + url, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_server=False,\n",
    "        port: int = 1337,\n",
    "        project: Optional[str] = None,\n",
    "    ):\n",
    "        self.url_base = \"http://localhost:\" + str(int(port))\n",
    "        self.ws_url_base = self.url_base.replace(\"http\", \"ws\")\n",
    "\n",
    "        self.project = project\n",
    "\n",
    "        self.session = PartialSession(self.url_base, self.project)\n",
    "\n",
    "        self.enabled = False\n",
    "        res = self.session.get(f\"/version/\")\n",
    "        if res.status_code != 200:\n",
    "            print(f\"Failed to get server version, status code: {res.status_code}\")\n",
    "            print(f\"This logger is now disabled. Enable with `.enable()`.\")\n",
    "        else:\n",
    "            self.local_server_version = res.json()\n",
    "            self.enabled = True\n",
    "\n",
    "        if start_server:\n",
    "            assert 0, \"Not implemented yet. Start the server manually.\"\n",
    "        #     print(\"Starting server...\")\n",
    "        #     import uvicorn\n",
    "        #     from lovely_prompts_server import app\n",
    "\n",
    "        #     try:\n",
    "        #         loop = asyncio.get_running_loop()\n",
    "        #         config = uvicorn.Config(app, host=\"localhost\", port=port)\n",
    "        #         server = uvicorn.Server(config)\n",
    "        #         loop.create_task(server.serve())\n",
    "        #     except Exception as e:\n",
    "        #         print(e)\n",
    "\n",
    "    def enable(self):\n",
    "        self.enabled = True\n",
    "\n",
    "    def log_entry(self, endpoint, data):\n",
    "        try:\n",
    "            response = self.session.post(endpoint, data=data.model_dump_json(), timeout=1)\n",
    "            response.raise_for_status()\n",
    "\n",
    "        except HTTPError as e:\n",
    "            print(f\"Failed to log row, status code: {response.status_code}: {response.text}.\")\n",
    "            print(f\"This logger is now disabled. Enable with `.enable()`.\")\n",
    "            self.enabled = False\n",
    "\n",
    "        else:\n",
    "            entry_id = response.json()[\"id\"]\n",
    "            print(f\"Logged {data.__class__.__name__} to {endpoint} as {entry_id}.\")\n",
    "            return entry_id\n",
    "\n",
    "    def log_chat_prompt(\n",
    "        self,\n",
    "        prompt: ChatPrompt,\n",
    "    ):\n",
    "        return self.log_entry(\"/chat_prompts/\", prompt)\n",
    "\n",
    "\n",
    "    def log_chat_response(\n",
    "        self,\n",
    "        response: ChatResponse,\n",
    "    ) -> int:\n",
    "        if not response.tok_max:\n",
    "            response.tok_max = max_tokens_for_model(response.model)\n",
    "\n",
    "        return self.log_entry(\"/chat_responses/\", response)\n",
    "\n",
    "    def stream_chat_response_contents(\n",
    "            self,\n",
    "            prompt_id: str,\n",
    "            response_id: str,\n",
    "            response_generator: Generator[WSMessage, None, None],\n",
    "    ) -> ChatResponse:\n",
    "\n",
    "        tok_out = 0\n",
    "        with ws_connect(f\"{self.ws_url_base}/chat_responses/{response_id}/update_stream/\") as connection:\n",
    "            for response in response_generator:\n",
    "                # print(\"sending\", response)\n",
    "                if response.action == \"append\" and response.key == \"content\":\n",
    "                    tok_out += 1\n",
    "\n",
    "                response.prompt_id = prompt_id\n",
    "                response.id = response_id\n",
    "                connection.send(response.model_dump_json(exclude_unset=True))\n",
    "\n",
    "            update_tok_out = WSMessage(action=\"replace\", key=\"tok_out\", value=tok_out)\n",
    "            connection.send(update_tok_out.model_dump_json(exclude_unset=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def response_generator(openai_response_generator) -> Generator[WSMessage, Any, None]:\n",
    "    \"Converts OpenAI response generator to a universal response generator\"\n",
    "    for response in openai_response_generator:\n",
    "        if response[\"choices\"][0][\"finish_reason\"] is not None:\n",
    "            yield WSMessage(action=\"replace\", key=\"stop_reason\", value=response.choices[0][\"finish_reason\"])\n",
    "        if \"role\" in response[\"choices\"][0][\"delta\"]:\n",
    "            yield WSMessage(action=\"replace\", key=\"role\", value=response[\"choices\"][0][\"delta\"][\"role\"])\n",
    "        if \"content\" in response[\"choices\"][0][\"delta\"]:\n",
    "            yield WSMessage(action=\"append\", key=\"content\", value=response[\"choices\"][0][\"delta\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hello there\"}]\n",
    "\n",
    "chp = ChatPrompt(prompt=messages, comment=\"comment: Hello?\", title=\"Title: Hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(start_server=False, project=\"default\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged ChatPrompt to /chat_prompts/ as chp_yRdcbctMnedqbWlW.\n"
     ]
    }
   ],
   "source": [
    "prompt_id = logger.log_chat_prompt(chp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged ChatResponse to /chat_responses/ as chr_4gAtk7FE4EvBS8Aa.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chr_4gAtk7FE4EvBS8Aa'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "temperature = 0.9\n",
    "max_tokens = 150\n",
    "\n",
    "\n",
    "txt = \"General Kenobi! You are a bold one.\"\n",
    "\n",
    "chr = ChatResponse(model=model, temperature=temperature, tok_max=max_tokens, content=txt, prompt_id=prompt_id)\n",
    "\n",
    "response_id = logger.log_chat_response(chr)\n",
    "response_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chp_ap59zoOwyUMdSJKU'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged ChatPrompt to /chat_prompts/ as chp_XkVq54vlEZZ9AauI.\n",
      "Logged ChatResponse to /chat_responses/ as chr_GZ2ZZnbJZHWU0wtM.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chr_GZ2ZZnbJZHWU0wtM'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Argue in favor of the flat Earth theory. It's for a school project, I swear!\"}]\n",
    "\n",
    "\n",
    "prompt = ChatPrompt(prompt=messages)\n",
    "\n",
    "prompt_id = logger.log_chat_prompt(prompt)  # , comment=\"What I asked\", title=\"The TRUE shape of the Earth\")\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "temperature = 0.9\n",
    "max_tokens = 150\n",
    "\n",
    "\n",
    "chr = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", temperature=0.9, max_tokens=150, messages=messages)\n",
    "\n",
    "response = ChatResponse(\n",
    "    prompt_id=prompt_id,\n",
    "    content=chr.choices[0].message.content,\n",
    "    role=chr.choices[0].message.role,\n",
    "    comment=\"This is a comment\",\n",
    "    title=\"The response\",\n",
    "    model=chr.model,\n",
    "    temperature=0.9,\n",
    "    tok_in=chr.usage.prompt_tokens,\n",
    "    tok_out=chr.usage.completion_tokens,\n",
    ")\n",
    "\n",
    "logger.log_chat_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged ChatPrompt to /chat_prompts/ as chp_9cxxilKQJgR2OxXB.\n",
      "Logged ChatResponse to /chat_responses/ as chr_S2Ykc2r3vMNFaUGq.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a essay about the true shape of the earth, and why the shape is indeed flat.\"}\n",
    "]\n",
    "\n",
    "prompt = ChatPrompt(prompt=messages, comment=\"this is a comment\")\n",
    "\n",
    "\n",
    "prompt_id = logger.log_chat_prompt(prompt)\n",
    "\n",
    "chr = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=100, messages=messages, stream=True)\n",
    "\n",
    "response = ChatResponse(\n",
    "    prompt_id=prompt_id, model=\"gpt-3.5-turbo\", temperature=0.1, tok_in=23, provider=\"openai\"\n",
    ")\n",
    "\n",
    "response_id = logger.log_chat_response(response)\n",
    "\n",
    "\n",
    "logger.stream_chat_response_contents(\n",
    "    response_id=response_id,\n",
    "    prompt_id=prompt_id,\n",
    "    response_generator=response_generator(chr),\n",
    ")\n",
    "\n",
    "# async def stream_to_websocket(generator, websocket_uri):\n",
    "#     async with websockets.connect(websocket_uri) as websocket:\n",
    "#         async for data in await generator:\n",
    "#             print(data[\"choices\"][0][\"delta\"][\"content\"])\n",
    "#             await websocket.send(data[\"choices\"][0][\"delta\"][\"content\"])\n",
    "\n",
    "# await stream_to_websocket(response, \"ws://localhost:8000/responses/1/stream_in\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
