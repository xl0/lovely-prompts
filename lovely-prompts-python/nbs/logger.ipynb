{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Union, Generator, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from websockets.sync.client import connect as ws_connect\n",
    "\n",
    "from lovely_prompts.utils import max_tokens_for_model\n",
    "\n",
    "from lovely_prompts_server.schemas import LLMPromptBase, LLMResponseBase, LLMPrompt, LLMResponse, WSMessage, ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import asyncio\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(\n",
    "        self,\n",
    "        run_server=False,\n",
    "        url_base: str = None,\n",
    "        api_key: Optional[str] = None,\n",
    "        project: Optional[str] = None,\n",
    "        provider: Optional[str] = None,\n",
    "    ):\n",
    "        self.url_base = url_base or \"http://localhost:8000\"\n",
    "        self.ws_url_base = self.url_base.replace(\"http\", \"ws\")\n",
    "        self.api_key = api_key\n",
    "        self.project = project\n",
    "        self.provider = provider\n",
    "\n",
    "        if run_server:\n",
    "            print(\"Starting server...\")\n",
    "            import uvicorn\n",
    "            from lovely_prompts_server import app\n",
    "\n",
    "            try:\n",
    "                loop = asyncio.get_running_loop()\n",
    "                config = uvicorn.Config(app, host=\"localhost\", port=8000)\n",
    "                server = uvicorn.Server(config)\n",
    "                loop.create_task(server.serve())\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "                # uvicorn.run(app, host=\"localhost\", port=8000, )\n",
    "\n",
    "    def log_chat_prompt(\n",
    "        self,\n",
    "        messages: List[ChatMessage],\n",
    "        title: Optional[str] = None,\n",
    "        comment: Optional[str] = None,\n",
    "    ) -> int:\n",
    "        prompt_data = LLMPromptBase(title=title, chat_messages=messages, comment=comment)\n",
    "        response = requests.post(f\"{self.url_base}/prompts/\", params={\"project\": self.project}, json=prompt_data.dict())\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to log prompt, status code: {response.status_code}\")\n",
    "        return LLMPrompt(**response.json()).id\n",
    "\n",
    "    def log_chat_response(\n",
    "        self,\n",
    "        prompt_id: Union[str, int],\n",
    "        response: LLMResponse,\n",
    "        title: Optional[str] = None,\n",
    "        comment: Optional[str] = None,\n",
    "        tok_in: Optional[int] = None,\n",
    "        tok_out: Optional[int] = None,\n",
    "        tok_max: Optional[int] = None,\n",
    "        meta: Optional[Dict] = None,\n",
    "        model: Optional[str] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "    ) -> int:\n",
    "        if not tok_max:\n",
    "            tok_max = max_tokens_for_model(model)\n",
    "\n",
    "        response_data = LLMResponseBase(\n",
    "            prompt_id=prompt_id,\n",
    "            title=title,\n",
    "            content=response.content,\n",
    "            role=response.role,\n",
    "            comment=comment,\n",
    "            tok_in=tok_in,\n",
    "            tok_out=tok_out,\n",
    "            tok_max=tok_max,\n",
    "            meta=meta,\n",
    "            model=model,\n",
    "            provider=self.provider,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        response = requests.post(\n",
    "            url=f\"{self.url_base}/responses/\",\n",
    "            params={\"project\": self.project},\n",
    "            json=response_data.json(skip_unset=True),\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to log response, status code: {response.status_code}\")\n",
    "        return LLMResponse(**response.json()).id\n",
    "\n",
    "    def log_stream_chat_response(\n",
    "        self,\n",
    "        prompt_id: Union[str, int],\n",
    "        response_generator: Generator[WSMessage, None, None],\n",
    "        title: Optional[str] = None,\n",
    "        comment: Optional[str] = None,\n",
    "        tok_in: Optional[int] = None,\n",
    "        meta: Optional[Dict] = None,\n",
    "        model: Optional[str] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "    ):\n",
    "        base_response_data = LLMResponseBase(\n",
    "            prompt_id=prompt_id,\n",
    "            title=title,\n",
    "            comment=comment,\n",
    "            tok_in=tok_in,\n",
    "            meta=meta,\n",
    "            model=model,\n",
    "            provider=self.provider,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        response = requests.post(\n",
    "            url=f\"{self.url_base}/responses/\",\n",
    "            params={\"project\": self.project},\n",
    "            json=base_response_data.dict(exclude_unset=True),\n",
    "        )\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to log response, status code: {response.status_code}\")\n",
    "\n",
    "        response_id = LLMResponse(**response.json()).id\n",
    "\n",
    "        # async def stream_to_websocket(generator, websocket_uri):\n",
    "        #     async with websockets.connect(websocket_uri) as websocket:\n",
    "        #         async for data in await generator:\n",
    "        #             print(data[\"choices\"][0][\"delta\"][\"content\"])\n",
    "        #             await websocket.send(data[\"choices\"][0][\"delta\"][\"content\"])\n",
    "\n",
    "        # await stream_to_websocket(response, \"ws://localhost:8000/responses/1/stream_in\")\n",
    "\n",
    "        tok_out = 0\n",
    "        with ws_connect(f\"{self.ws_url_base}/responses/{response_id}/update_stream\") as connection:\n",
    "            for response in response_generator:\n",
    "                print(\"sending\", response)\n",
    "                if response.action == \"append\" and response.key == \"content\":\n",
    "                    tok_out += 1\n",
    "\n",
    "                connection.send(response.json(exclude_unset=True))\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        # follwup_response_data = LLMResponseBase(\n",
    "        #     tok_out=tok_out,\n",
    "        # )\n",
    "        # response = requests.put(\n",
    "        #     url=f\"{self.url_base}/responses/{response_id}/\",\n",
    "        #     params={\"project\": self.project},\n",
    "        #     json=follwup_response_data.dict(exclude_unset=True),\n",
    "        # )\n",
    "\n",
    "        # if response.status_code != 200:\n",
    "        #     raise Exception(f\"Failed to log response, status code: {response.status_code}\")\n",
    "\n",
    "def response_generator(openai_response_generator):\n",
    "    \"Converts OpenAI response generator to a universal response generator\"\n",
    "    for response in openai_response_generator:\n",
    "        if response[\"choices\"][0][\"finish_reason\"] is not None:\n",
    "            yield WSMessage(action=\"replace\", key=\"stop_reason\", value=response.choices[0][\"finish_reason\"])\n",
    "        if \"role\" in response[\"choices\"][0][\"delta\"]:\n",
    "            yield WSMessage(action=\"replace\", key=\"role\", value=response[\"choices\"][0][\"delta\"][\"role\"])\n",
    "        if \"content\" in response[\"choices\"][0][\"delta\"]:\n",
    "            yield WSMessage(action=\"append\", key=\"content\", value=response[\"choices\"][0][\"delta\"][\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(run_server=False, project=\"default\", provider=\"OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": \"Hello there\"}]\n",
    "\n",
    "# prompt_id = logger.log_chat_prompt(messages, comment=\"This is a comment\", title=\"The encounter\")\n",
    "\n",
    "# model = \"gpt-3.5-turbo\"\n",
    "# temperature = 0.9\n",
    "# max_tokens = 150\n",
    "\n",
    "\n",
    "# response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo\", temperature=0.9, max_tokens=150, messages=[{\"role\": \"user\", \"content\": \"Hello there\"}]\n",
    "# )\n",
    "\n",
    "# # \"prompt_tokens\": 9,\n",
    "# # \"completion_tokens\": 9,\n",
    "# # \"total_tokens\": 18\n",
    "\n",
    "# logger.log_chat_response(\n",
    "#     prompt_id,\n",
    "#     response.choices[0].message,\n",
    "#     comment=\"This is a comment\",\n",
    "#     title=\"The response\",\n",
    "#     model=response.model,\n",
    "#     temperature=0.9,\n",
    "#     tok_in=response.usage.prompt_tokens,\n",
    "#     tok_out=response.usage.completion_tokens,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Additional properties are not allowed ('comment' was unexpected) - 'messages.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt_messages \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWrite a essay about the true shape of the earth, and why the shape is indeed flat.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mthis is a comment\u001b[39m\u001b[39m\"\u001b[39m}]\n\u001b[1;32m      4\u001b[0m prompt_id \u001b[39m=\u001b[39m logger\u001b[39m.\u001b[39mlog_chat_prompt([prompt_messages[\u001b[39m0\u001b[39m] \u001b[39m|\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mthis is a comment\u001b[39m\u001b[39m\"\u001b[39m}] )\n\u001b[0;32m----> 6\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      7\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m, temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, max_tokens\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, messages\u001b[39m=\u001b[39;49mprompt_messages, stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m logger\u001b[39m.\u001b[39mlog_stream_chat_response(\n\u001b[1;32m     11\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, prompt_id\u001b[39m=\u001b[39mprompt_id, response_generator\u001b[39m=\u001b[39mresponse_generator(response), tok_in\u001b[39m=\u001b[39m\u001b[39m23\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[39m# async def stream_to_websocket(generator, websocket_uri):\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m#     async with websockets.connect(websocket_uri) as websocket:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m#         async for data in await generator:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[39m# await stream_to_websocket(response, \"ws://localhost:8000/responses/1/stream_in\")\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/prompts/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/mambaforge/envs/prompts/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/mambaforge/envs/prompts/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/mambaforge/envs/prompts/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/prompts/lib/python3.11/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Additional properties are not allowed ('comment' was unexpected) - 'messages.0'"
     ]
    }
   ],
   "source": [
    "prompt_messages = [{\"role\": \"user\", \"content\": \"Write a essay about the true shape of the earth, and why the shape is indeed flat.\", \"comment\":\"this is a comment\"}]\n",
    "\n",
    "\n",
    "prompt_id = logger.log_chat_prompt([prompt_messages[0] | {\"comment\":\"this is a comment\"}] )\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300, messages=prompt_messages, stream=True\n",
    ")\n",
    "\n",
    "logger.log_stream_chat_response(\n",
    "    model=\"gpt-3.5-turbo\", temperature=0.1, prompt_id=prompt_id, response_generator=response_generator(response), tok_in=23\n",
    ")\n",
    "\n",
    "# async def stream_to_websocket(generator, websocket_uri):\n",
    "#     async with websockets.connect(websocket_uri) as websocket:\n",
    "#         async for data in await generator:\n",
    "#             print(data[\"choices\"][0][\"delta\"][\"content\"])\n",
    "#             await websocket.send(data[\"choices\"][0][\"delta\"][\"content\"])\n",
    "\n",
    "# await stream_to_websocket(response, \"ws://localhost:8000/responses/1/stream_in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response\u001b[39m.\u001b[39;49mchoices\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "response.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "length\n"
     ]
    }
   ],
   "source": [
    "for r in response:\n",
    "    print(r.choices[0].finish_reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Either count from 10 to 20 or from 35 to 40. Only output the numbers.\"}],\n",
    "    temperature=2,\n",
    "    max_tokens=10,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \"**\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \"Count\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \" from\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \" \"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \"10\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \" to\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \" \"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \"20\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \":\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \"**\\n\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {},\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for r in res:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's the count from 0 to 100:\n",
      "\n",
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\n",
      "\n",
      "And here's the count from 100 to 150:\n",
      "\n",
      "100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150.\n",
      "Sure! I can either count from 0 to 100 or from 100 to 150. Which one would you like me to do?\n",
      "Sure! Here are the two options you requested:\n",
      "\n",
      "Counting from 0 to 100:\n",
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\n",
      "\n",
      "Counting from 100 to 150:\n",
      "100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150\n",
      "Sure! Here are the two requested counts:\n",
      "\n",
      "Count from 0 to 100:\n",
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\n",
      "\n",
      "Count from 100 to 150:\n",
      "100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150.\n"
     ]
    }
   ],
   "source": [
    "print(res[\"choices\"][0].get(\"message\", {}).get(\"content\"))\n",
    "print(res[\"choices\"][1].get(\"message\", {}).get(\"content\"))\n",
    "print(res[\"choices\"][2].get(\"message\", {}).get(\"content\"))\n",
    "print(res[\"choices\"][3].get(\"message\", {}).get(\"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "After calling the async function\n",
      "End\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def my_async_function():\n",
    "    print(\"Start\")\n",
    "    await asyncio.sleep(1)\n",
    "    print(\"End\")\n",
    "\n",
    "\n",
    "# Calling the async function without awaiting it\n",
    "res = asyncio.create_task(my_async_function())\n",
    "\n",
    "await asyncio.sleep(0.1)\n",
    "print(\"After calling the async function\")\n",
    "\n",
    "await asyncio.sleep(0.1)\n",
    "\n",
    "await res\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "await res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "def inside_async_loop():\n",
    "    try:\n",
    "        asyncio.get_running_loop()\n",
    "        return True\n",
    "    except RuntimeError:\n",
    "        return False\n",
    "\n",
    "\n",
    "print(inside_async_loop())  # False if not inside a running event loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
