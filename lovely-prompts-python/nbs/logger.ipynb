{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Union, Generator, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from websockets.sync.client import connect as ws_connect\n",
    "\n",
    "from lovely_prompts.utils import max_tokens_for_model\n",
    "\n",
    "from lovely_prompts_server.models import (\n",
    "    WSMessage,\n",
    "    ChatMessage,\n",
    "    ChatPrompt,\n",
    "    ChatResponse,\n",
    "    CompletionPrompt,\n",
    "    CompletionResponse,\n",
    "\n",
    "    ChatPromptModel,\n",
    "    CompletionPromptModel,\n",
    "    ChatResponseModel,\n",
    "    CompletionResponseModel,\n",
    "    # LocalChatPromptModel,\n",
    "    # LocalCompletionPromptModel,\n",
    "    # LocalChatResponseModel,\n",
    "    # LocalCompletionResponseModel,\n",
    ")\n",
    "from lovely_prompts_server.orm.local import (\n",
    "    LocalChatPromptSchema,\n",
    "    LocalCompletionPromptSchema,\n",
    "    LocalChatResponseSchema,\n",
    "    LocalCompletionResponseSchema,\n",
    "    LocalBase,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import uuid\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import Session, sessionmaker\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nanoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "import threading\n",
    "\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ChatPromptModel'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptModel.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_row(sm, row_schema, row_model, endpoint, url_base: str, project: str):\n",
    "    with sm() as session:\n",
    "        backlog = session.query(row_schema).filter(row_schema.synced == False).all()\n",
    "        for row in backlog:\n",
    "            model = row_model.model_validate(row)\n",
    "            print(model.model_dump_json())\n",
    "\n",
    "            response = requests.post(f\"{url_base}/{endpoint}/\", params={\"project\": project}, data=model.model_dump_json())\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to log row, status code: {response.status_code}: {response.text}.\")\n",
    "                continue\n",
    "\n",
    "            row.synced = True\n",
    "            print(f\"Logged {row_model.__name__.replace('Model', '')} {row.id} to {url_base}/{endpoint}\")\n",
    "        session.commit()\n",
    "\n",
    "\n",
    "def sync_worker(sm: sessionmaker, q: Queue, url_base: str, project: str):\n",
    "    print(\"worker: Starting\")\n",
    "    while True:\n",
    "        try:\n",
    "            message = q.get()\n",
    "            print(\"worker: Got message\", message)\n",
    "            log_row(sm, LocalChatPromptSchema, ChatPromptModel, \"chat_prompts\", url_base, project)\n",
    "            log_row(sm, LocalCompletionPromptSchema, CompletionPromptModel, \"completion_prompts\", url_base, project)\n",
    "            log_row(sm, LocalChatResponseSchema, ChatResponseModel, \"chat_responses\", url_base, project)\n",
    "            log_row(sm, LocalCompletionResponseSchema, CompletionResponseModel, \"completion_responses\", url_base, project)\n",
    "\n",
    "            q.task_done()\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_nonoid = partial(nanoid.generate, size=16)\n",
    "\n",
    "\n",
    "def make_id(entry_class=None):\n",
    "    \"\"\"Generate a unique ID for a DB entry. Also used to generate the name of the DB file.\"\"\"\n",
    "\n",
    "    prefixes = {\n",
    "        ChatPrompt: \"chp_\",\n",
    "        CompletionPrompt: \"cop_\",\n",
    "        ChatResponse: \"chr_\",\n",
    "        CompletionResponse: \"cor_\",\n",
    "    }\n",
    "\n",
    "    alphabet = string.digits + string.ascii_lowercase + string.ascii_uppercase\n",
    "\n",
    "    return (prefixes[entry_class] if entry_class else \"\") + generate_nonoid(alphabet=alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chp_BIlazvAo1qNDYMn8', 'IolSd3ZIv5EUmjyI')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_id(ChatPrompt), make_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.put(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(\n",
    "        self,\n",
    "        run_server=False,\n",
    "        url_base: str = None,\n",
    "        api_key: Optional[str] = None,\n",
    "        project: Optional[str] = None,\n",
    "        provider: Optional[str] = None,\n",
    "    ):\n",
    "        self.url_base = url_base or \"http://localhost:8000\"\n",
    "        self.ws_url_base = self.url_base.replace(\"http\", \"ws\")\n",
    "        self.api_key = api_key\n",
    "        self.project = project\n",
    "        self.provider = provider\n",
    "\n",
    "        if run_server:\n",
    "            print(\"Starting server...\")\n",
    "            import uvicorn\n",
    "            from lovely_prompts_server import app\n",
    "\n",
    "            try:\n",
    "                loop = asyncio.get_running_loop()\n",
    "                config = uvicorn.Config(app, host=\"localhost\", port=8000)\n",
    "                server = uvicorn.Server(config)\n",
    "                loop.create_task(server.serve())\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "                # uvicorn.run(app, host=\"localhost\", port=8000, )\n",
    "\n",
    "    def get_sessionmaker(self):\n",
    "        if not hasattr(self, \"sessionmaker\"):\n",
    "            dir = Path(\".\")\n",
    "\n",
    "            os.makedirs(dir / \"_lovely_prompts\", exist_ok=True)\n",
    "            # Find the highest number in the filenames\n",
    "            highest_number = max(\n",
    "                [int(re.findall(r\"\\d+\", f)[0]) for f in os.listdir(dir / \"_lovely_prompts\") if f.startswith(\"run_\")]\n",
    "                + [0]\n",
    "            )\n",
    "\n",
    "            sqlite_file = dir / \"_lovely_prompts\" / f\"run_{highest_number + 1}_{make_id()}.db\"\n",
    "            engine = create_engine(f\"sqlite:///{sqlite_file}\")\n",
    "\n",
    "            LocalBase.metadata.create_all(bind=engine)\n",
    "            self.sessionmaker = sessionmaker(bind=engine, autoflush=True)\n",
    "            self.queue = Queue()\n",
    "            self.sync_thread = threading.Thread(\n",
    "                target=sync_worker, args=(self.sessionmaker, self.queue, self.url_base, self.project), daemon=True\n",
    "            )\n",
    "            self.sync_thread.start()\n",
    "\n",
    "        return self.sessionmaker\n",
    "\n",
    "    def log_chat_prompt(\n",
    "        self,\n",
    "        prompt: ChatPrompt,\n",
    "    ):\n",
    "        maker = self.get_sessionmaker()\n",
    "        id = make_id(ChatPrompt)\n",
    "        with maker() as session:\n",
    "            db_prompt = LocalChatPromptSchema(**prompt.model_dump(exclude_unset=True), id=id)\n",
    "            session.add(db_prompt)\n",
    "            session.commit()\n",
    "\n",
    "        self.queue.put(\"add\")\n",
    "        return id\n",
    "\n",
    "    def log_chat_response(\n",
    "        self,\n",
    "        response: ChatResponse,\n",
    "    ) -> int:\n",
    "        if not response.tok_max:\n",
    "            tok_max = max_tokens_for_model(response.model)\n",
    "\n",
    "        maker = self.get_sessionmaker()\n",
    "        id = make_id(ChatResponse)\n",
    "        with maker() as session:\n",
    "            db_response = LocalChatResponseSchema(**response.model_dump(exclude_unset=True), id=id)\n",
    "            session.add(db_response)\n",
    "            session.commit()\n",
    "\n",
    "        self.queue.put(\"add\")\n",
    "        return id\n",
    "\n",
    "    def stream_chat_response_contents(\n",
    "            self,\n",
    "            prompt_id: str,\n",
    "            response_id: str,\n",
    "            response_generator: Generator[WSMessage, None, None],\n",
    "    ) -> ChatResponse:\n",
    "        \n",
    "\n",
    "        for response in response_generator:\n",
    "            print(\"sending\", response)\n",
    "            if response.action == \"append\" and response.key == \"content\":\n",
    "                tok_out += 1\n",
    "\n",
    "            response.prompt_id = prompt_id\n",
    "            response.response_id = response_id\n",
    "\n",
    "            self.queue.put(response)\n",
    "\n",
    "            connection.send(response.model_dump_json(exclude_unset=True))\n",
    "\n",
    "\n",
    "        for response in response_generator:\n",
    "            print(response)\n",
    "\n",
    "        import websockets\n",
    "\n",
    "        # async def stream_to_websocket(generator, websocket_uri):\n",
    "        #     async with websockets.connect(websocket_uri) as websocket:\n",
    "        #         async for data in await generator:\n",
    "        #             print(data[\"choices\"][0][\"delta\"][\"content\"])\n",
    "        #             await websocket.send(data[\"choices\"][0][\"delta\"][\"content\"])\n",
    "\n",
    "        # stream_to_websocket(response_generator, \"ws://localhost:8000/responses/1/stream_in\")\n",
    "\n",
    "        tok_out = 0\n",
    "        with ws_connect(f\"{self.ws_url_base}/chat_responsess/{response_id}/update_stream\") as connection:\n",
    "            for response in response_generator:\n",
    "                print(\"sending\", response)\n",
    "                if response.action == \"append\" and response.key == \"content\":\n",
    "                    tok_out += 1\n",
    "\n",
    "                response.prompt_id = prompt_id\n",
    "                response.response_id = response_id\n",
    "                connection.send(response.model_dump_json(exclude_unset=True))\n",
    "\n",
    "        followup_response_data = ChatResponse(\n",
    "            tok_out=tok_out,\n",
    "        )\n",
    "\n",
    "        response = requests.put(\n",
    "            url=f\"{self.url_base}/responses/{response_id}/\",\n",
    "            params={\"project\": self.project},\n",
    "            json=followup_response_data.model_dump(),\n",
    "        )\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to log response, status code: {response.status_code}\")\n",
    "        \n",
    "\n",
    "\n",
    "    #     # follwup_response_data = LLMResponseBase(\n",
    "    #     #     tok_out=tok_out,\n",
    "    #     # )\n",
    "    #     # response = requests.put(\n",
    "    #     #     url=f\"{self.url_base}/responses/{response_id}/\",\n",
    "    #     #     params={\"project\": self.project},\n",
    "    #     #     json=follwup_response_data.dict(exclude_unset=True),\n",
    "    #     # )\n",
    "\n",
    "    #     # if response.status_code != 200:\n",
    "    #     #     raise Exception(f\"Failed to log response, status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "def response_generator(openai_response_generator) -> Generator[WSMessage, Any, None]:\n",
    "    \"Converts OpenAI response generator to a universal response generator\"\n",
    "    for response in openai_response_generator:\n",
    "        if response[\"choices\"][0][\"finish_reason\"] is not None:\n",
    "            yield WSMessage(action=\"replace\", key=\"stop_reason\", value=response.choices[0][\"finish_reason\"])\n",
    "        if \"role\" in response[\"choices\"][0][\"delta\"]:\n",
    "            yield WSMessage(action=\"replace\", key=\"role\", value=response[\"choices\"][0][\"delta\"][\"role\"])\n",
    "        if \"content\" in response[\"choices\"][0][\"delta\"]:\n",
    "            yield WSMessage(action=\"append\", key=\"content\", value=response[\"choices\"][0][\"delta\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hello there\"}]\n",
    "\n",
    "chp = ChatPrompt(prompt=messages, comment=\"comment: Hello?\", title=\"Title: Hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(run_server=False, project=\"default\", provider=\"OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: Starting\n",
      "worker: Got message add\n"
     ]
    }
   ],
   "source": [
    "prompt_id = logger.log_chat_prompt(chp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chp_Z1aivu3hRNctlvpX\",\"created\":\"2023-08-18T08:44:16\",\"updated\":\"2023-08-18T08:44:16\",\"synced\":false,\"title\":\"Title: Hello there\",\"comment\":\"comment: Hello?\",\"prompt\":[{\"role\":\"user\",\"content\":\"Hello there\"}],\"responses\":[]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chr_YKCmallDAnIzdelJ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged ChatPrompt chp_Z1aivu3hRNctlvpX to http://localhost:8000/chat_prompts\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "temperature = 0.9\n",
    "max_tokens = 150\n",
    "\n",
    "\n",
    "txt = \"General Kenobi! You are a bold one.\"\n",
    "\n",
    "chr = ChatResponse(model=model, temperature=temperature, tok_max=max_tokens, content=txt, prompt_id=prompt_id)\n",
    "\n",
    "response_id = logger.log_chat_response(chr)\n",
    "response_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chp_Z1aivu3hRNctlvpX'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chr_YKCmallDAnIzdelJ\",\"created\":\"2023-08-18T08:44:16\",\"updated\":\"2023-08-18T08:44:16\",\"synced\":false,\"prompt_id\":\"chp_Z1aivu3hRNctlvpX\",\"content\":\"General Kenobi! You are a bold one.\",\"tok_max\":150,\"model\":\"gpt-3.5-turbo\",\"temperature\":0.9}\n",
      "Logged ChatResponse chr_YKCmallDAnIzdelJ to http://localhost:8000/chat_responses\n",
      "worker: Got message add\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": \"Argue in favor of the flat Earth theory. It's for a school project, I swear!\"}]\n",
    "\n",
    "\n",
    "# prompt = ChatPrompt(prompt=messages)\n",
    "\n",
    "# prompt_id = logger.log_chat_prompt(prompt)  # , comment=\"What I asked\", title=\"The TRUE shape of the Earth\")\n",
    "\n",
    "# model = \"gpt-3.5-turbo\"\n",
    "# temperature = 0.9\n",
    "# max_tokens = 150\n",
    "\n",
    "\n",
    "# chr = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", temperature=0.9, max_tokens=150, messages=messages)\n",
    "\n",
    "# response = ChatResponse(\n",
    "#     prompt_id=prompt_id,\n",
    "#     content=chr.choices[0].message.content,\n",
    "#     role=chr.choices[0].message.role,\n",
    "#     comment=\"This is a comment\",\n",
    "#     title=\"The response\",\n",
    "#     model=chr.model,\n",
    "#     temperature=0.9,\n",
    "#     tok_in=chr.usage.prompt_tokens,\n",
    "#     tok_out=chr.usage.completion_tokens,\n",
    "# )\n",
    "\n",
    "# logger.log_chat_response(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: Got message add\n",
      "{\"id\":\"chp_0lth1T25GcppDcn2\",\"created\":\"2023-08-18T08:44:43\",\"updated\":\"2023-08-18T08:44:43\",\"synced\":false,\"comment\":\"this is a comment\",\"prompt\":[{\"role\":\"user\",\"content\":\"Write a essay about the true shape of the earth, and why the shape is indeed flat.\"}],\"responses\":[]}\n",
      "Logged ChatPrompt chp_0lth1T25GcppDcn2 to http://localhost:8000/chat_prompts\n",
      "worker: Got message add\n",
      "{\"id\":\"chr_lOEmeeL5NABIrKnC\",\"created\":\"2023-08-18T08:44:44\",\"updated\":\"2023-08-18T08:44:44\",\"synced\":false,\"prompt_id\":\"chp_0lth1T25GcppDcn2\",\"tok_in\":23,\"tok_max\":800,\"model\":\"gpt-3.5-turbo\",\"temperature\":0.1,\"provider\":\"openai\"}\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for WSMessage\nid\n  Field required [type=missing, input_value={'action': 'replace', 'ke...', 'value': 'assistant'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.1/v/missing\nprompt_id\n  Field required [type=missing, input_value={'action': 'replace', 'ke...', 'value': 'assistant'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.1/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 25\u001b[0m\n\u001b[1;32m     14\u001b[0m response \u001b[39m=\u001b[39m ChatResponse(\n\u001b[1;32m     15\u001b[0m     prompt_id\u001b[39m=\u001b[39mprompt_id,\n\u001b[1;32m     16\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     provider\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mopenai\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     tok_max\u001b[39m=\u001b[39m\u001b[39m800\u001b[39m)\n\u001b[1;32m     22\u001b[0m response_id \u001b[39m=\u001b[39m logger\u001b[39m.\u001b[39mlog_chat_response(response)\n\u001b[0;32m---> 25\u001b[0m logger\u001b[39m.\u001b[39;49mstream_chat_response_contents(\n\u001b[1;32m     26\u001b[0m     response_id\u001b[39m=\u001b[39;49mresponse_id,\n\u001b[1;32m     27\u001b[0m     prompt_id\u001b[39m=\u001b[39;49mprompt_id,\n\u001b[1;32m     28\u001b[0m     response_generator\u001b[39m=\u001b[39;49mresponse_generator(\u001b[39mchr\u001b[39;49m),\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[39m# async def stream_to_websocket(generator, websocket_uri):\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m#     async with websockets.connect(websocket_uri) as websocket:\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m#         async for data in await generator:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[39m# await stream_to_websocket(response, \"ws://localhost:8000/responses/1/stream_in\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 94\u001b[0m, in \u001b[0;36mLogger.stream_chat_response_contents\u001b[0;34m(self, prompt_id, response_id, response_generator)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstream_chat_response_contents\u001b[39m(\n\u001b[1;32m     87\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m     88\u001b[0m         prompt_id: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     89\u001b[0m         response_id: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     90\u001b[0m         response_generator: Generator[WSMessage, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m],\n\u001b[1;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResponse:\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mfor\u001b[39;00m response \u001b[39min\u001b[39;00m response_generator:\n\u001b[1;32m     95\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msending\u001b[39m\u001b[39m\"\u001b[39m, response)\n\u001b[1;32m     96\u001b[0m         \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39maction \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mkey \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[13], line 162\u001b[0m, in \u001b[0;36mresponse_generator\u001b[0;34m(openai_response_generator)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[39myield\u001b[39;00m WSMessage(action\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m\"\u001b[39m, key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstop_reason\u001b[39m\u001b[39m\"\u001b[39m, value\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mfinish_reason\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdelta\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39myield\u001b[39;00m WSMessage(action\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mreplace\u001b[39;49m\u001b[39m\"\u001b[39;49m, key\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m, value\u001b[39m=\u001b[39;49mresponse[\u001b[39m\"\u001b[39;49m\u001b[39mchoices\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mdelta\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdelta\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    164\u001b[0m     \u001b[39myield\u001b[39;00m WSMessage(action\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m\"\u001b[39m, key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m, value\u001b[39m=\u001b[39mresponse[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdelta\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/envs/prompts/lib/python3.11/site-packages/pydantic/main.py:159\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    158\u001b[0m __tracebackhide__ \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m __pydantic_self__\u001b[39m.\u001b[39;49m__pydantic_validator__\u001b[39m.\u001b[39;49mvalidate_python(data, self_instance\u001b[39m=\u001b[39;49m__pydantic_self__)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for WSMessage\nid\n  Field required [type=missing, input_value={'action': 'replace', 'ke...', 'value': 'assistant'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.1/v/missing\nprompt_id\n  Field required [type=missing, input_value={'action': 'replace', 'ke...', 'value': 'assistant'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.1/v/missing"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged ChatResponse chr_lOEmeeL5NABIrKnC to http://localhost:8000/chat_responses\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a essay about the true shape of the earth, and why the shape is indeed flat.\"}\n",
    "]\n",
    "\n",
    "prompt = ChatPrompt(prompt=messages, comment=\"this is a comment\")\n",
    "\n",
    "\n",
    "prompt_id = logger.log_chat_prompt(prompt)\n",
    "\n",
    "chr = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300, messages=messages, stream=True\n",
    ")\n",
    "\n",
    "response = ChatResponse(\n",
    "    prompt_id=prompt_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,\n",
    "    tok_in=23,\n",
    "    provider=\"openai\",\n",
    "    tok_max=800)\n",
    "\n",
    "response_id = logger.log_chat_response(response)\n",
    "\n",
    "\n",
    "logger.stream_chat_response_contents(\n",
    "    response_id=response_id,\n",
    "    prompt_id=prompt_id,\n",
    "    response_generator=response_generator(chr),\n",
    ")\n",
    "\n",
    "# async def stream_to_websocket(generator, websocket_uri):\n",
    "#     async with websockets.connect(websocket_uri) as websocket:\n",
    "#         async for data in await generator:\n",
    "#             print(data[\"choices\"][0][\"delta\"][\"content\"])\n",
    "#             await websocket.send(data[\"choices\"][0][\"delta\"][\"content\"])\n",
    "\n",
    "# await stream_to_websocket(response, \"ws://localhost:8000/responses/1/stream_in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response\u001b[39m.\u001b[39;49mchoices\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "chr.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "length\n"
     ]
    }
   ],
   "source": [
    "for cor in chr:\n",
    "    print(cor.choices[0].finish_reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Either count from 10 to 20 or from 35 to 40. Only output the numbers.\"}],\n",
    "    temperature=2,\n",
    "    max_tokens=10,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \"**\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \"Count\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \" from\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \" \"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \"10\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \" to\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \" \"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \"20\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \":\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \"**\\n\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7fi42XEsHS2cAinzbL1pu14Bmjg89\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1690175186,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {},\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for r in res:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's the count from 0 to 100:\n",
      "\n",
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\n",
      "\n",
      "And here's the count from 100 to 150:\n",
      "\n",
      "100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150.\n",
      "Sure! I can either count from 0 to 100 or from 100 to 150. Which one would you like me to do?\n",
      "Sure! Here are the two options you requested:\n",
      "\n",
      "Counting from 0 to 100:\n",
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\n",
      "\n",
      "Counting from 100 to 150:\n",
      "100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150\n",
      "Sure! Here are the two requested counts:\n",
      "\n",
      "Count from 0 to 100:\n",
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\n",
      "\n",
      "Count from 100 to 150:\n",
      "100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150.\n"
     ]
    }
   ],
   "source": [
    "print(res[\"choices\"][0].get(\"message\", {}).get(\"content\"))\n",
    "print(res[\"choices\"][1].get(\"message\", {}).get(\"content\"))\n",
    "print(res[\"choices\"][2].get(\"message\", {}).get(\"content\"))\n",
    "print(res[\"choices\"][3].get(\"message\", {}).get(\"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "After calling the async function\n",
      "End\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def my_async_function():\n",
    "    print(\"Start\")\n",
    "    await asyncio.sleep(1)\n",
    "    print(\"End\")\n",
    "\n",
    "\n",
    "# Calling the async function without awaiting it\n",
    "res = asyncio.create_task(my_async_function())\n",
    "\n",
    "await asyncio.sleep(0.1)\n",
    "print(\"After calling the async function\")\n",
    "\n",
    "await asyncio.sleep(0.1)\n",
    "\n",
    "await res\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "await res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "def inside_async_loop():\n",
    "    try:\n",
    "        asyncio.get_running_loop()\n",
    "        return True\n",
    "    except RuntimeError:\n",
    "        return False\n",
    "\n",
    "\n",
    "print(inside_async_loop())  # False if not inside a running event loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
