{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Union\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from lovely_prompts_server.schemas import ChatPromptBase, ChatResponseBase, ChatPrompt, ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PromptBase(BaseModel):\n",
    "#     title: str = Field(None, example=\"The encounter\")\n",
    "#     prompt: Union[List[Dict], str] = Field(None, example={\"messages\": [{\"role\": \"user\", \"content\": \"Hello there\"}]})\n",
    "#     comment: str = Field(None, example=\"This is a comment\")\n",
    "#     project: str = Field(None, example=\"my-awesome-project\")\n",
    "\n",
    "#     class Config:\n",
    "#         orm_mode = True\n",
    "\n",
    "\n",
    "# class ResponseBase(BaseModel):\n",
    "#     prompt_id: int = Field(None, example=1)\n",
    "#     title: str = Field(None, example=\"The response\")\n",
    "#     response: Dict = Field(None, example=\"General Kenobi!!\")\n",
    "#     comment: str = Field(None, example=\"This is a response comment\")\n",
    "#     tok_in: int = Field(None, example=10)\n",
    "#     tok_out: int = Field(None, example=20)\n",
    "#     tok_max: int = Field(None, example=8000)\n",
    "#     meta: Dict = Field(None)\n",
    "#     model: str = Field(None, example=\"gpt-3.5-turbo\")\n",
    "#     temperature: float = Field(None, example=0.7)\n",
    "#     provider: str = Field(None, example=\"openai\")\n",
    "\n",
    "\n",
    "# class Response(ResponseBase):\n",
    "#     id: int\n",
    "#     # prompt: PromptBase = Field(..., alias=\"prompt\")\n",
    "\n",
    "\n",
    "# class Prompt(PromptBase):\n",
    "#     id: int\n",
    "#     responses: List[ResponseBase] = Field([], alias=\"responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(\n",
    "        self,\n",
    "        run_server=False,\n",
    "        url_base: str = None,\n",
    "        api_key: Optional[str] = None,\n",
    "        project: Optional[str] = None,\n",
    "        provider: Optional[str] = None,\n",
    "    ):\n",
    "        self.url_base = url_base or \"http://localhost:8000\"\n",
    "        self.api_key = api_key\n",
    "        self.project = project\n",
    "        self.provider = provider\n",
    "\n",
    "        if run_server:\n",
    "            print(\"Starting server...\")\n",
    "            import uvicorn\n",
    "            from lovely_prompts_server import app\n",
    "\n",
    "            try:\n",
    "                loop = asyncio.get_running_loop()\n",
    "                config = uvicorn.Config(app, host=\"localhost\", port=8000)\n",
    "                server = uvicorn.Server(config)\n",
    "                loop.create_task(server.serve())\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "                # uvicorn.run(app, host=\"localhost\", port=8000, )\n",
    "\n",
    "    def log_chat_prompt(\n",
    "        self,\n",
    "        messages: List[Dict],\n",
    "        title: Optional[str] = None,\n",
    "        comment: Optional[str] = None,\n",
    "        project: Optional[str] = None,\n",
    "    ) -> int:\n",
    "        prompt_data = ChatPromptBase(\n",
    "            title=title, messages=messages, comment=comment, project=project if project else self.project\n",
    "        )\n",
    "        response = requests.post(f\"{self.url_base}/prompts/\", json=prompt_data.dict())\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to log prompt, status code: {response.status_code}\")\n",
    "        return ChatPrompt(**response.json()).id\n",
    "\n",
    "    def log_chat_response(\n",
    "        self,\n",
    "        prompt_id: Union[str, int],\n",
    "        response: Dict,\n",
    "        title: Optional[str] = None,\n",
    "        comment: Optional[str] = None,\n",
    "        tok_in: Optional[int] = None,\n",
    "        tok_out: Optional[int] = None,\n",
    "        tok_max: Optional[int] = None,\n",
    "        meta: Optional[Dict] = None,\n",
    "        model: Optional[str] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "    ) -> int:\n",
    "        response_data = ChatResponseBase(\n",
    "            prompt_id=prompt_id,\n",
    "            title=title,\n",
    "            response=response,\n",
    "            comment=comment,\n",
    "            tok_in=tok_in,\n",
    "            tok_out=tok_out,\n",
    "            tok_max=tok_max,\n",
    "            meta=meta,\n",
    "            model=model,\n",
    "            provider=self.provider,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        response = requests.post(f\"{self.url_base}/responses/\", json=response_data.dict())\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to log response, status code: {response.status_code}\")\n",
    "        return ChatResponse(**response.json()).id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# chat = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo\", temperature=0.9, max_tokens=150, messages=[{\"role\": \"user\", \"content\": \"Hello there\"}]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(run_server=False, project=\"Test project\", provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hello there\"}]\n",
    "\n",
    "prompt_id = logger.log_chat_prompt(messages, comment=\"This is a comment\", title=\"The encounter\")\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", temperature=0.9, max_tokens=150, messages=[{\"role\": \"user\", \"content\": \"Hello there\"}]\n",
    ")\n",
    "\n",
    "# \"prompt_tokens\": 9,\n",
    "# \"completion_tokens\": 9,\n",
    "# \"total_tokens\": 18\n",
    "\n",
    "logger.log_chat_response(\n",
    "    prompt_id,\n",
    "    response.choices[0].message,\n",
    "    comment=\"This is a comment\",\n",
    "    title=\"The response\",\n",
    "    model=response.model,\n",
    "    temperature=0.7,\n",
    "    tok_in=response.usage.prompt_tokens,\n",
    "    tok_out=response.usage.completion_tokens,\n",
    "    tok_max=8000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "def inside_async_loop():\n",
    "    try:\n",
    "        asyncio.get_running_loop()\n",
    "        return True\n",
    "    except RuntimeError:\n",
    "        return False\n",
    "\n",
    "\n",
    "print(inside_async_loop())  # False if not inside a running event loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
